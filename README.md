

---

### ğŸ’¡ What the project does

* Loads a dataset (in my case: `cat-facts.txt` with 150 fun cat facts ğŸ±).
* Generates **embeddings** for each text chunk using Ollama (`nomic-embed-text`).
* Stores these embeddings in an in-memory vector database.
* When a user asks a question â†’ it embeds the query â†’ retrieves the most relevant chunks using **cosine similarity** â†’ sends them to a language model (`llama3`) to generate a grounded answer.

---

### âš™ï¸ The workflow

1. **Data Loading** â†’ read and split the dataset.
2. **Embedding & Storage** â†’ convert chunks into embeddings with Ollama.
3. **Retriever** â†’ `retrieve(query)` finds the top-N relevant chunks.
4. **Flask Web App** â†’ a simple web UI where you type your question and get an answer.
5. **Chat History** â†’ every Q\&A is stored and displayed like a chat conversation.

---

### ğŸ–¥ï¸ The result

A small web chatbot where you can ask things like:
ğŸ‘‰ *â€œTell me something interesting about catsâ€*
and it responds using the actual dataset rather than hallucinating.



